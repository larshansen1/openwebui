# Open WebUI Production Stack
# Deployed via GitHub Actions CI/CD pipeline
services:
  postgres:
    image: postgres:17
    container_name: openwebui-postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: openwebui
      POSTGRES_USER: openwebui
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - ./postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U openwebui" ]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - openwebui-network

  qdrant:
    image: qdrant/qdrant:v1.16.2
    container_name: openwebui-qdrant
    restart: unless-stopped
    environment:
      QDRANT__SERVICE__API_KEY: ${QDRANT_API_KEY:-}
    volumes:
      - ./qdrant_data:/qdrant/storage
    ports:
      - "127.0.0.1:6333:6333" # Only accessible locally
    healthcheck:
      test: [ "CMD-SHELL", "timeout 1 bash -c '</dev/tcp/localhost/6333' || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - openwebui-network

  ollama:
    build:
      context: .
      dockerfile: Dockerfile.ollama
      args:
        EMBEDDING_MODEL: ${RAG_EMBEDDING_MODEL}
    container_name: openwebui-ollama
    restart: unless-stopped
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    environment:
      EMBEDDING_MODEL: ${RAG_EMBEDDING_MODEL}
      OLLAMA_MAX_LOADED_MODELS: 1
      OLLAMA_NUM_PARALLEL: 2
    volumes:
      - ./ollama_data:/root/.ollama
    ports:
      - "127.0.0.1:11434:11434" # Only accessible locally
    networks:
      - openwebui-network

  mcp-server:
    image: ghcr.io/open-webui/mcpo:latest
    container_name: openwebui-mcp-server
    restart: unless-stopped
    user: "1000:1000"  # Run as user 1000 to match vault permissions (syncthing PUID/PGID)
    entrypoint: [ "/app/entrypoint.sh" ]
    environment:
      MCP_API_KEY: ${MCP_API_KEY}
      BRAVE_API_KEY: ${BRAVE_API_KEY:-}
      HOME: /tmp  # Set writable home for uvx/npx cache directories
      NPM_CONFIG_CACHE: /tmp/.npm  # npm cache directory
      UV_CACHE_DIR: /tmp/.cache/uv  # uv cache directory
    volumes:
      # Mount only template file, not entire directory - generated config stays in container
      - ./mcp_config/config.template.json:/app/config/config.template.json:ro
      - ./mcp-entrypoint.sh:/app/entrypoint.sh:ro
      # Mount Obsidian vault for mcp-obsidian server (writable for full CRUD operations)
      - ${OBSIDIAN_VAULT_PATH}:/vault
    ports:
      - "127.0.0.1:8000:8000" # Only accessible locally
    networks:
      - openwebui-network
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
    healthcheck:
      test: [ "CMD-SHELL", "curl -f http://localhost:8000/openapi.json || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  openwebui:
    build:
      context: .
      dockerfile: Dockerfile.openwebui
    container_name: openwebui
    restart: unless-stopped
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    depends_on:
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_healthy
      ollama:
        condition: service_started
      # obsidian-mcp:  # REMOVED: Now using mcp-obsidian via mcpo proxy
      #   condition: service_healthy
      mcp-server:
        condition: service_healthy
    environment:
      # Database Configuration
      DATABASE_URL: postgresql://openwebui:${POSTGRES_PASSWORD}@postgres:5432/openwebui

      # Qdrant Configuration
      ENABLE_RAG_WEB_SEARCH: "true"
      ENABLE_RAG_LOCAL_WEB_FETCH: "true"
      VECTOR_DB: qdrant
      QDRANT_URI: http://qdrant:6333
      QDRANT_API_KEY: ${QDRANT_API_KEY:-}

      # Multi-user Configuration
      WEBUI_AUTH: "true"
      WEBUI_SECRET_KEY: ${WEBUI_SECRET_KEY}

      # OpenAI API Configuration (OpenRouter)
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      OPENAI_API_BASE_URL: ${OPENAI_API_BASE_URL}

      # Ollama Configuration
      OLLAMA_BASE_URL: http://ollama:11434
      RAG_EMBEDDING_ENGINE: ollama
      RAG_EMBEDDING_MODEL: ${RAG_EMBEDDING_MODEL}

      # Chunking Configuration for all-minilm (512 token context)
      # Using conservative values to prevent context overflow
      CHUNK_SIZE: 400
      CHUNK_OVERLAP: 50
      PDF_EXTRACT_IMAGES: "false"

      # Limit concurrent requests to prevent "too many open files"
      RAG_EMBEDDING_OPENAI_BATCH_SIZE: 1
      DOCS_PIPELINE_CONCURRENCY: 2

      # Connection pooling limits
      AIOHTTP_CLIENT_TIMEOUT: 300
      AIOHTTP_CLIENT_TIMEOUT_TOTAL: 600

      #Enable Auto-Tagging. Using Default prompt
      ENABLE_TAGS_GENERATION: "true"

      # MCP API Key (needed for TOOL_SERVER_CONNECTIONS substitution)
      MCP_API_KEY: ${MCP_API_KEY}

      # OpenTelemetry Configuration for Monitoring
      ENABLE_OTEL: "true"
      ENABLE_OTEL_TRACES: "true"
      ENABLE_OTEL_METRICS: "true"
      OTEL_EXPORTER_OTLP_ENDPOINT: "http://otel-collector:4317"
      OTEL_EXPORTER_OTLP_INSECURE: "true"
      OTEL_SERVICE_NAME: "open-webui"

      # MCP Server Connection Configuration
      TOOL_SERVER_CONNECTIONS: |
        [
          {
            "type": "openapi",
            "url": "http://mcp-server:8000/brave-search",
            "spec_type": "url",
            "spec": "",
            "path": "openapi.json",
            "auth_type": "bearer",
            "key": "${MCP_API_KEY}",
            "config": { "enable": true },
            "info": {
              "id": "brave-search",
              "name": "Brave Search",
              "description": "Web and local search using Brave Search API"
            }
          },
          {
            "type": "openapi",
            "url": "http://mcp-server:8000/time",
            "spec_type": "url",
            "spec": "",
            "path": "openapi.json",
            "auth_type": "bearer",
            "key": "${MCP_API_KEY}",
            "config": { "enable": true },
            "info": {
              "id": "time",
              "name": "Time Tools",
              "description": "Get current time and convert between timezones"
            }
          },
          {
            "type": "openapi",
            "url": "http://mcp-server:8000/fetch",
            "spec_type": "url",
            "spec": "",
            "path": "openapi.json",
            "auth_type": "bearer",
            "key": "${MCP_API_KEY}",
            "config": { "enable": true },
            "info": {
              "id": "fetch",
              "name": "Fetch Tool",
              "description": "Fetch content from URLs"
            }
          },
          {
            "type": "openapi",
            "url": "http://mcp-server:8000/obsidian",
            "spec_type": "url",
            "spec": "",
            "path": "openapi.json",
            "auth_type": "bearer",
            "key": "${MCP_API_KEY}",
            "config": { "enable": true },
            "info": {
              "id": "obsidian",
              "name": "Obsidian Vault",
              "description": "Read, write, search, and manage Obsidian vault notes"
            }
          }
        ]
    volumes:
      - ./openwebui_data:/app/backend/data
    ports:
      - "127.0.0.1:3000:8080" # Only accessible locally, nginx will proxy
    networks:
      - openwebui-network

  # ============================================================================
  # Monitoring Stack
  # ============================================================================

  otel-collector:
    image: otel/opentelemetry-collector:0.142.0
    container_name: openwebui-otel-collector
    restart: unless-stopped
    command: [ "--config=/etc/otel-collector-config.yml" ]
    volumes:
      - ./monitoring/otel-collector-config.yml:/etc/otel-collector-config.yml:ro
    ports:
      - "4317:4317" # OTLP gRPC receiver (internal only)
      - "8889:8889" # Prometheus exporter (internal only)
    networks:
      - openwebui-network
    # Note: No healthcheck - collector doesn't include wget/curl, and it's working fine

  prometheus:
    image: prom/prometheus:v3.8.1
    container_name: openwebui-prometheus
    restart: unless-stopped
    user: "65534:65534" # Run as nobody user to match file permissions
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=15d'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/alerts:/etc/prometheus/alerts:ro
      - ./monitoring/secrets:/etc/prometheus/secrets:ro
      - ./prometheus_data:/prometheus
    ports:
      - "127.0.0.1:9090:9090" # Prometheus web UI (localhost only)
    networks:
      - openwebui-network
    healthcheck:
      test: [ "CMD", "wget", "--spider", "-q", "http://localhost:9090/-/healthy" ]
      interval: 30s
      timeout: 10s
      retries: 3

  grafana:
    image: grafana/grafana:12.1.1
    container_name: openwebui-grafana
    restart: unless-stopped
    user: "472:472" # Run as grafana user to match file permissions
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
      - GF_SERVER_ROOT_URL=http://localhost:3001
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - ./grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
    ports:
      - "127.0.0.1:3001:3000" # Grafana web UI (localhost only, access via SSH tunnel)
    networks:
      - openwebui-network
    depends_on:
      - prometheus
    healthcheck:
      test: [ "CMD", "wget", "--spider", "-q", "http://localhost:3000/api/health" ]
      interval: 30s
      timeout: 10s
      retries: 3

  alertmanager:
    image: prom/alertmanager:v0.29.0
    container_name: openwebui-alertmanager
    restart: unless-stopped
    user: "65534:65534" # Run as nobody user to match file permissions
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    volumes:
      - ./monitoring/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - ./alertmanager_data:/alertmanager
    ports:
      - "127.0.0.1:9093:9093" # Alertmanager web UI (localhost only)
    networks:
      - openwebui-network
    healthcheck:
      test: [ "CMD", "wget", "--spider", "-q", "http://localhost:9093/-/healthy" ]
      interval: 30s
      timeout: 10s
      retries: 3

  telegram-forwarder:
    build: ./monitoring/telegram-forwarder
    container_name: openwebui-telegram-forwarder
    restart: unless-stopped
    environment:
      - TELEGRAM_BOT_TOKEN=${TELEGRAM_BOT_TOKEN}
      - TELEGRAM_CHAT_ID=${TELEGRAM_CHAT_ID}
    networks:
      - openwebui-network
    healthcheck:
      test: [ "CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8080/health')" ]
      interval: 30s
      timeout: 10s
      retries: 3

networks:
  openwebui-network:
    name: openwebui_openwebui-network

volumes:
  # Syncthing data volume for Obsidian vault (production)
  # Create with: docker volume create syncthing-data
  # Uncomment for production:
  # syncthing-data:
  #   external: true

  # Placeholder to keep volumes section valid
  postgres-data:
